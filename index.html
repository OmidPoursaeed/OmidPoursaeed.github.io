<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37.1" />
  <meta name="author" content="Omid Poursaeed">
  <meta name="description" content="PhD Candidate at Cornell University &amp; Cornell Tech">

  
  <link rel="alternate" hreflang="en-us" href="https://omidpoursaeed.github.io/">

  
  


  

  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/academic1.css">
  

  

  
  <link rel="alternate" href="https://omidpoursaeed.github.io/index.xml" type="application/rss+xml" title="Omid Poursaeed">
  <link rel="feed" href="https://omidpoursaeed.github.io/index.xml" type="application/rss+xml" title="Omid Poursaeed">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://omidpoursaeed.github.io/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Omid Poursaeed">
  <meta property="og:url" content="https://omidpoursaeed.github.io/">
  <meta property="og:title" content="Omid Poursaeed">
  <meta property="og:description" content="PhD Candidate at Cornell University &amp; Cornell Tech">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2016-04-20T00:00:00&#43;00:00">
  

  

  <title>Omid Poursaeed</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Omid Poursaeed</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about" data-target="#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications_selected" data-target="#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#experience" data-target="#experience">
            
            <span>Experience</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#talks" data-target="#talks">
            
            <span>Presentations</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#professional_service" data-target="#professional_service">
            
            <span>Service</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#patents" data-target="#patents">
            
            <span>Patents</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#contact" data-target="#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>



<span id="homepage" style="display: none"></span>


  





  
  
  
  <section id="about" class="home-section">
    <div class="container">
      



<div class="row" itemprop="author" itemscope itemtype="http://schema.org/Person" itemref="person-email person-address">
  <div class="col-xs-12 col-md-4">
    <div id="profile">

      
      <div class="portrait" style="background-image: url('https://omidpoursaeed.github.io/img/omid1.jpg');"></div>
      <meta itemprop="image" content="https://omidpoursaeed.github.io/img/omid1.jpg">
      

      <div class="portrait-title">
        <h2 itemprop="name">Omid Poursaeed</h2>
        <h3 itemprop="jobTitle">PhD Candidate at Cornell University &amp; Cornell Tech</h3>

        
        

        
        <h3 itemprop="worksFor" itemscope itemtype="http://schema.org/Organization">
          
          <span itemprop="name"></span>
          
        </h3>
        
      </div>

      <link itemprop="url" href="https://omidpoursaeed.github.io/">

      <ul class="network-icon" aria-hidden="true">
        
        
        <li>
          <a itemprop="sameAs" href="mailto:op63@cornell.edu" target="_blank" rel="noopener">
            <i class="fa fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://scholar.google.com/citations?user=Ugw9DX0AAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noopener">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.linkedin.com/in/omidpoursaeed/" target="_blank" rel="noopener">
            <i class="fa fa-linkedin big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.researchgate.net/profile/Omid_Poursaeed" target="_blank" rel="noopener">
            <i class="ai ai-researchgate big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://github.com/OmidPoursaeed" target="_blank" rel="noopener">
            <i class="fa fa-github big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.facebook.com/omid.poursaeed" target="_blank" rel="noopener">
            <i class="fa fa-facebook big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-xs-12 col-md-8" itemprop="description">

    

<h1 id="about">About</h1>

<p>I am a PhD candidate at <a href="https://www.cornell.edu" target="_blank">Cornell University</a> and <a href="https://tech.cornell.edu" target="_blank">Cornell Tech</a> working with <a href="http://blogs.cornell.edu/techfaculty/serge-belongie/" target="_blank">Serge Belongie</a>. My research interests are in Computer Vision, Generative Models and 3D Deep Learning. I am a recipient of Jacobs Fellowship from Cornell University and DLI Doctoral Fellowship from Cornell Tech.
<br/>
<br/>
You can find my CV <a href="pdf/Curriculum_Vitae.pdf" target="_blank">here</a>.<br />
<br/>
<br/>
<br/>
<img src="/img/Logos_2019.png" alt="static/img/omid.png" /></p>


    <div class="row">

      

      

    </div>
  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="publications_selected" class="home-section">
    <div class="container">
      



<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Selected Publications</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    

    
      
    

    
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/hybrid/">
        <img src="/img/hybrid.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/hybrid/" itemprop="url">Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a novel neural architecture for representing 3D surfaces by harnessing complementary explicit and implicit shape representations. We make these two representations synergistic by introducing novel consistency losses. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir Kim
        
      </div>

      <div class="pub-publication">
        ECCV,&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/HybridNet.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/hybrid.bib">
  Cite
</button>








<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Hybrid.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2007.10294" target="_blank" rel="noopener">
  Arxiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/HybridNet_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/self-supervised/">
        <img src="/img/self_supervised.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/self-supervised/" itemprop="url">Self-supervised Learning of Point Clouds via Orientation Estimation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        While deep neural networks have achieved impressive results on point cloud learning tasks, they require massive amounts of manually labeled data. In this paper we leverage 3D self-supervision for learning downstream tasks on point clouds with fewer labels. A point cloud can be rotated in infinitely many ways, which provides a rich label-free source for self-supervision. We consider the auxiliary task of predicting rotations that in turn leads to useful features for other tasks. Using experiments on ShapeNet and ModelNet, we demonstrate that our approach outperforms the state-of-the-art. Moreover, features learned by our model are complementary to other self-supervised methods and combining them leads to further performance improvement.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, Vladimir Kim
        
      </div>

      <div class="pub-publication">
        3DV,&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Self_supervised_Point_Clouds.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/self-supervised.bib">
  Cite
</button>








<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Self_supervised_slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2008.00305" target="_blank" rel="noopener">
  Arxiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/fine-grained-unrestricted/">
        <img src="/img/faces2.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/fine-grained-unrestricted/" itemprop="url">Fine-grained Generation of Unrestricted Adversarial Examples</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a novel approach for generating unrestricted adversarial examples by manipulating fine-grained aspects of image generation. Unlike existing unrestricted attacks that typically hand-craft geometric transformations, we learn stylistic and stochastic modifications leveraging state-of-the-art generative models. Our approach can be used for targeted and non-targeted unrestricted attacks on classification, semantic segmentation and object detection models. Our attacks can bypass certified defenses, yet our adversarial images look indistinguishable from natural images as verified by human evaluation. Moreover, we demonstrate that adversarial training with our examples improves performance of the model on clean images without requiring any modifications to the architecture.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, Ser-Nam Lim
        
      </div>

      <div class="pub-publication">
        ICLR (Under review),&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Fine_grained_Unrestricted_Adversarial_Examples.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/fine-grained-unrestricted.bib">
  Cite
</button>








<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Fine-grained_Adversarial_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1911.09058" target="_blank" rel="noopener">
  Arxiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Unrestricted_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/few-shot/">
        <img src="/img/Few_shot.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/few-shot/" itemprop="url">Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Davis Wertheimer, Omid Poursaeed, Bharath Hariharan
        
      </div>

      <div class="pub-publication">
        ICLR (Under review),&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Few_shot.pdf" target="_blank" rel="noopener">
  PDF
</a>














<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Few_shot_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/generative-neural-networks-for-2.5d-character-animation/">
        <img src="/img/Neural_Puppet.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/generative-neural-networks-for-2.5d-character-animation/" itemprop="url">Neural Puppet: Generative Layered Cartoon Characters</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a learning based method for generating new animations of a cartoon character given a few example images. We express pose changes as a deformation of a layered 2.5D template mesh, and devise a novel architecture that learns to predict mesh deformations matching the template to a target image. In addition to coarse poses, character appearance also varies due to shading, out-of-plane motions, and artistic effects. We capture these subtle changes by applying an image translation network to refine the mesh rendering. Our generative model can be used to synthesize in-between frames and to create data-driven deformation. Our template fitting procedure outperforms state-of-the-art generic techniques for detecting image correspondences.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Vladimir Kim, Eli Shechtman, Jun Saito, Serge Belongie
        
      </div>

      <div class="pub-publication">
        WACV,&nbsp;2019.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/generative-neural-networks-for-2.5D-character-animation.bib">
  Cite
</button>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet_Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1910.02060" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/differential-privacy/">
        <img src="/img/DP.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/differential-privacy/" itemprop="url">Differential Privacy has Disparate Impact on Model Accuracy</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Differential privacy (DP) is a popular mechanism for training machine learning models with bounded leakage about the presence of specific points in the training data. The cost of differential privacy is a reduction in the modelâ€™s accuracy. We demonstrate that in the neural networks trained using differentially private stochastic gradient descent (DP-SGD), this cost is not borne equally: accuracy of DP models drops much more for the underrepresented classes and subgroups. We demonstrate this effect for a variety of tasks and models, including sentiment analysis of text and image classification. We then explain why DP training mechanisms such as gradient clipping and noise addition have disproportionate effect on the underrepresented and more complex subgroups.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Eugene Bagdasaryan, Omid Poursaeed, Vitaly Shmatikov
        
      </div>

      <div class="pub-publication">
        NeurIPS,&nbsp;2019.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Differential_privacy.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/differential-privacy.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/ebagdasa/differential-privacy-vs-fairness" target="_blank" rel="noopener">
  Code
</a>






<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/neurips_diffprivacy_fairness.pdf" target="_blank" rel="noopener">
  Poster
</a>






      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/deep-fundamental-matrix-estimation-without-correspondences/">
        <img src="/img/Deep-FM.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/deep-fundamental-matrix-estimation-without-correspondences/" itemprop="url">Deep Fundamental Matrix Estimation without Correspondences</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Estimating fundamental matrices is a classic problem in computer vision. Traditional methods rely heavily on the correctness of estimated key-point correspondences, which can be noisy and unreliable. As a result, it is difficult for these methods to handle image pairs with large occlusion or significantly different camera poses. In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences. New modules and layers are introduced in order to preserve mathematical properties of the fundamental matrix as a homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance of the proposed model on the KITTI dataset, and show that they achieve competitive performance with traditional methods without the need for extracting correspondences.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Guandao Yang, Aditya Prakash, Hanqing Jiang, Qiuren Fang, Bharath Hariharan, Serge Belongie
        
      </div>

      <div class="pub-publication">
        ECCV,&nbsp;2018.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Deep_Fundamental_Matrix_Estimation.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/deep-fundamental-matrix-estimation-without-correspondences.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/file/d/1vcvzmZ0diUS0tpt20_TDwQ-Tl3XgG41m/view?usp=sharing" target="_blank" rel="noopener">
  Code
</a>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/F_Matrix_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/pdf/1810.01575.pdf" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/open?id=1sYJsb9Aa2E6cXxzelSNZtdjCQgwuKczy" target="_blank" rel="noopener">
  Poster
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/generative-adversarial-perturbations/">
        <img src="/img/GAP_Architecture.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/generative-adversarial-perturbations/" itemprop="url">Generative Adversarial Perturbations</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool trained models. Our approach can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. We improve the state-of-the-art performance in universal perturbations by leveraging generative models in lieu of current iterative methods. Our attacks are considerably faster than iterative and optimization-based methods at inference time. Moreover, we are the first to present effective targeted universal perturbations.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie
        
      </div>

      <div class="pub-publication">
        CVPR,&nbsp;2018.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/generative-adversarial-perturbations.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/OmidPoursaeed/Generative_Adversarial_Perturbations" target="_blank" rel="noopener">
  Code
</a>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAP_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1712.02328" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/open?id=1fFIJM2ETysPmAfra7qRA3Wc25cZuIrnD" target="_blank" rel="noopener">
  Poster
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAP_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/stacked-generative-adversarial-networks/">
        <img src="/img/sgan.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/stacked-generative-adversarial-networks/" itemprop="url">Stacked Generative Adversarial Networks</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a novel generative model which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie
        
      </div>

      <div class="pub-publication">
        CVPR,&nbsp;2017.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/SGAN_CVPR.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/stacked-generative-adversarial-networks.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/xunhuang1995/SGAN" target="_blank" rel="noopener">
  Code
</a>






<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/SGAN_poster.pdf" target="_blank" rel="noopener">
  Poster
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/SGAN_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1612.04357" target="_blank" rel="noopener">
  ArXiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/vision-based-real-estate-price-estimation/">
        <img src="/img/vision-based-narrow-2.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/vision-based-real-estate-price-estimation/" itemprop="url">Vision Based Real Estate Price Estimation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Several online real estate database companies provide automatic estimation of market values for houses using a proprietary formula. Although these estimates are often close to the actual sale prices, in some cases they are highly inaccurate. One of the key factors that affects the value of a house is its interior and exterior appearance, which is not considered in calculating these estimates. In this paper, we evaluate the impact of visual characteristics of a house on its market value. Using deep convolutional neural networks on a large dataset of photos of home interiors and exteriors, we develop a novel framework for automated value assessment using these photos in addition to other home characteristics. By applying our proposed method for price estimation to a new dataset of real estate photos and metadata, we show that it outperforms Zillow&rsquo;s estimates.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Tomas Matera, Serge Belongie
        
      </div>

      <div class="pub-publication">
        Machine Vision and Applications,&nbsp;2017.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="http://rdcu.be/KzC9" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/vision-based-real-estate-price-estimation.bib">
  Cite
</button>



<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/file/d/0BxDIywue_VABY1dRcFVvZ3BodnM/view?usp=sharing" target="_blank" rel="noopener">
  Dataset
</a>









<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1707.05489" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Vision_based_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    

  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="experience" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Experience</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <ul>
<li>Research Intern: <strong>Adobe Research</strong> with <a href="http://www.vovakim.com/" target="_blank">Vladimir Kim</a> and <a href="https://techmatt.github.io" target="_blank">Matthew Fisher</a>, Summer 2019 <br />
<br /></li>
<li>Research Intern: <strong>Facebook AI</strong> with <a href="https://www.linkedin.com/in/sernam/" target="_blank">SerNam Lim</a>, Fall &amp; Winter 2019 <br />
<br /></li>
<li>Research Intern: <strong>Adobe Research</strong> with <a href="http://www.vovakim.com/" target="_blank">Vladimir Kim</a> and <a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a>, Summer 2018 <br />
<br /></li>
<li>Research Intern: <strong>eBay Research</strong> with <a href="http://kylezheng.org/" target="_blank">Shuai Zheng</a>, <a href="http://www.cs.unc.edu/~hadi/" target="_blank">Hadi Kiapour</a> and <a href="https://www.linkedin.com/in/rpiramuthu/" target="_blank">Robinson Piramuthu</a>, Summer 2017</li>
</ul>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="talks" class="home-section">
    <div class="container">
      




<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Presentations</h1>
    
    
  </div>
  <div class="col-xs-12 col-md-8">
    <p><ul>
<li><p><a href="https://docs.google.com/presentation/d/1Z-Srx-elUDYD6iKnHF6F5XN5z_ZzKAqQG_U_kw7-Q4M/edit?usp=sharing" target="_blank">Learning to Generate and Animate for Augmented Reality</a>: Invited talk at Facebook Reality Labs</p></li>

<li><p><a href="https://docs.google.com/presentation/d/1aRHXatmiHcXutxX88LkqGyA8AFnWBRb3Gc2DYbJvGCA/edit?usp=sharing" target="_blank">Hybrid Generative Models for 2D and 3D Computer Vision</a>: Invited talk at Apple &amp; Snapchat</p></li>

<li><p><a href="https://docs.google.com/presentation/d/1QjcOEsWqPhBh5MvL1VwCc_ti9VPLJg4FJUgkZOg7TjU/edit?usp=sharing" target="_blank">DeepFakes and Adversarial Examples: Fooling Humans and Machines</a>: <a href="https://36ef64cb-137f-4a56-9f1f-edccbcdef5aa.usrfiles.com/ugd/36ef64_a3ebec11f15c4b12b71db78c087a5f2e.pdf" target="_blank">Digital Life Seminar Series</a></p></li>
</ul>
</p>

    
  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="professional_service" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Service</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <p>Reviewer for CVPR, NeurIPS, ICCV, ECCV, ICLR, AAAI, IEEE TPAMI, IEEE Transactions on Multimedia, ACM Transactions on Privacy and Security, IEEE Transactions on Industrial Electronics <br />
<br />
Program Committee Member for CVPR Workshop on Adversarial Machine Learning</p>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="patents" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Patents</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <ul>
<li><a href="https://patents.google.com/patent/US20190286950A1/en" target="_blank">Generating a digital image using a generative adversarial network</a>, with Shuai Zheng, Hadi Kiapour and Robinson Piramuthu
<br />
<br /></li>
<li><a href="https://patents.google.com/patent/US20200265294A1/en" target="_blank">Object animation using Generative Neural Networks</a>, with Vladimir Kim, Jun Saito and Eli Shechtman
<br />
<br /></li>
<li>Learning Hybrid Shape Representations (pending), with Vladimir Kim, Matthew Fisher and Noam Aigerman</li>
</ul>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="contact" class="home-section">
    <div class="container">
      




<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Contact</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    

    <ul class="fa-ul" itemscope>

      
      <li>
        <i class="fa-li fa fa-envelope fa-2x" aria-hidden="true"></i>
        <span id="person-email" itemprop="email"><a href="mailto:op63@cornell.edu">op63@cornell.edu</a></span>
      </li>
      

      

      

      

      

      

      
      <li>
        <i class="fa-li fa fa-map-marker fa-2x" aria-hidden="true"></i>
        <span id="person-address" itemprop="address">NW 3-5-09, 1 West Loop Road, New York, NY 10044, USA.</span>
      </li>
      

      

    </ul>

    

  </div>
</div>

    </div>
  </section>
  



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Omid Poursaeed &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>


