<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37.1" />
  <meta name="author" content="Omid Poursaeed">
  <meta name="description" content="Research Scientist at Meta AI">

  
  <link rel="alternate" hreflang="en-us" href="https://omidpoursaeed.github.io/">

  
  


  

  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/academic1.css">
  

  

  
  <link rel="alternate" href="https://omidpoursaeed.github.io/index.xml" type="application/rss+xml" title="Omid Poursaeed">
  <link rel="feed" href="https://omidpoursaeed.github.io/index.xml" type="application/rss+xml" title="Omid Poursaeed">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://omidpoursaeed.github.io/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Omid Poursaeed">
  <meta property="og:url" content="https://omidpoursaeed.github.io/">
  <meta property="og:title" content="Omid Poursaeed">
  <meta property="og:description" content="Research Scientist at Meta AI">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2016-04-20T00:00:00&#43;00:00">
  

  

  <title>Omid Poursaeed</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Omid Poursaeed</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about" data-target="#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications_selected" data-target="#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#experience" data-target="#experience">
            
            <span>Experience</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#talks" data-target="#talks">
            
            <span>Presentations</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#professional_service" data-target="#professional_service">
            
            <span>Service</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#patents" data-target="#patents">
            
            <span>Patents</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#contact" data-target="#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>



<span id="homepage" style="display: none"></span>


  





  
  
  
  <section id="about" class="home-section">
    <div class="container">
      



<div class="row" itemprop="author" itemscope itemtype="http://schema.org/Person" itemref="person-email person-address">
  <div class="col-xs-12 col-md-4">
    <div id="profile">

      
      <div class="portrait" style="background-image: url('https://omidpoursaeed.github.io/img/omid1.jpg');"></div>
      <meta itemprop="image" content="https://omidpoursaeed.github.io/img/omid1.jpg">
      

      <div class="portrait-title">
        <h2 itemprop="name">Omid Poursaeed</h2>
        <h3 itemprop="jobTitle">Research Scientist at Meta AI</h3>

        
        

        
        <h3 itemprop="worksFor" itemscope itemtype="http://schema.org/Organization">
          
          <span itemprop="name"></span>
          
        </h3>
        
      </div>

      <link itemprop="url" href="https://omidpoursaeed.github.io/">

      <ul class="network-icon" aria-hidden="true">
        
        
        <li>
          <a itemprop="sameAs" href="mailto:op63@cornell.edu" target="_blank" rel="noopener">
            <i class="fa fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://scholar.google.com/citations?user=Ugw9DX0AAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noopener">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.linkedin.com/in/omidpoursaeed/" target="_blank" rel="noopener">
            <i class="fa fa-linkedin big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.researchgate.net/profile/Omid_Poursaeed" target="_blank" rel="noopener">
            <i class="ai ai-researchgate big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://github.com/OmidPoursaeed" target="_blank" rel="noopener">
            <i class="fa fa-github big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.facebook.com/omid.poursaeed" target="_blank" rel="noopener">
            <i class="fa fa-facebook big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-xs-12 col-md-8" itemprop="description">

    

<h1 id="about">About</h1>

<p><br/>
I am a Research Scientist at <a href="https://ai.facebook.com" target="_blank">Meta AI</a>. Before joining Meta, I obtained my PhD from <a href="https://www.cornell.edu" target="_blank">Cornell University</a> with <a href="https://www.belongielab.org/team/" target="_blank">Serge Belongie</a>. My research interests include Computer Vision, Multi-modal Learning and Generative Models.
<br/>
<br/>
<br style="line-height: 1px" />
<img src="/img/Logos_2022.png" alt="static/img/omid.png" /></p>


    <div class="row">

      

      

    </div>
  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="publications_selected" class="home-section">
    <div class="container">
      



<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Selected Publications</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    

    
      
    

    
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/pacl/">
        <img src="/img/PACL.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/pacl/" itemprop="url">Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We introduce Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP&rsquo;s contrastive loss, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. Using pre-trained CLIP encoders with PACL, we are able to set the state-of-the-art on the task of open vocabulary zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC, Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also applicable to image-level predictions and when used with a CLIP backbone, provides a general improvement in zero-shot classification accuracy compared to CLIP.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        J. Mukhoti, T.Y. Lin, Omid Poursaeed, R. Wang, A. Shah, P. Torr, S. Lim
        
      </div>

      <div class="pub-publication">
        CVPR (Highlight),&nbsp;2023.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/PACL.pdf" target="_blank" rel="noopener">
  PDF
</a>














<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2212.04994" target="_blank" rel="noopener">
  ArXiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/smvit/">
        <img src="/img/SMViT.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/smvit/" itemprop="url">Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Modern multi-stage vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, this added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is actually unnecessary. By pretraining with a strong pretext task (MAE), we can strip out all the bells and whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Simple MViT, an extremely simple multi-stage vision transformer that is more accurate than previous models while being significantly faster both at inference and during training.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        C. Ryali, Y. Hu, D. Bolya, C. Wei, H. Fan, B. Huang, V. Aggarwal, A. Chowdhury, Omid Poursaeed, J. Hoffman, J. Malik, Y. Li, C. Feichtenhofer
        
      </div>

      <div class="pub-publication">
        ICML (Oral),&nbsp;2023.
      </div>

      <div class="pub-links">
        








<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/facebookresearch/hiera" target="_blank" rel="noopener">
  Code
</a>










<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2306.00989" target="_blank" rel="noopener">
  ArXiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/unified-detection-tracking/">
        <img src="/img/TriVD.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/unified-detection-tracking/" itemprop="url">A Unified Model for Tracking and Image-Video Object Detection</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Recent developments in deep learning have pushed the performance of image OD to new heights by learning-based, data-driven approaches. On the other hand, video OD remains less explored, mostly due to much more expensive data annotation needs. At the same time, Multi-Object Tracking (MOT) shares similar spirits with video OD. However, most MOT datasets are class-specific, which constrains a model’s flexibility to perform tracking on other objects. We propose TrIVD (Tracking and Image-Video Detection), the first framework that unifies image OD, video OD, and MOT within one end-to-end model. Experiments demonstrate that TrIVD achieves state-of-the-art performance across all image/video OD and MOT tasks.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        P. Liu, R. Wang, P. Zhang, Omid Poursaeed, Y. Zhou, X. Cao, S. Roy, A. Shah, S. Lim
        
      </div>

      <div class="pub-publication">
        Under Review,&nbsp;2023.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/TriVD.pdf" target="_blank" rel="noopener">
  PDF
</a>














<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2211.11077" target="_blank" rel="noopener">
  ArXiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/kts/">
        <img src="/img/kts.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/kts/" itemprop="url">Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        A common approach to process long videos is applying a short-form video model over uniformly sampled clips of fixed temporal length and aggregating the outputs. In this paper, we aim to provide a generic and adaptive sampling approach for long-form videos in lieu of the de facto uniform sampling. We formulate a task-agnostic, unsupervised, and scalable approach based on Kernel Temporal Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our method on long-form video understanding tasks such as video classification and temporal action localization, showing consistent gains over existing approaches and achieving state-of-the-art performance on long-form video modeling.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        A. Aflal, S. Shukla, Omid Poursaeed, P. Zhang, A. Shah, S. Lim
        
      </div>

      <div class="pub-publication">
        ICCVW,&nbsp;2023.
      </div>

      <div class="pub-links">
        


















      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/uni_at/">
        <img src="/img/UniAT.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/uni_at/" itemprop="url">Universal Pyramid Adversarial Training for Improved ViT Performance</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Pyramid Adversarial training has been shown to be very effective for improving clean accuracy and robustness of vision transformers. However, due to the iterative nature of adversarial training, the technique is up to 7 times more expensive than standard training. To make the method more efficient, we propose Universal Pyramid Adversarial training, where we learn a single pyramid adversarial pattern shared across the whole dataset instead of the sample-wise patterns. We decrease the computational cost of Pyramid Adversarial training by up to 70 percent while retaining the majority of its benefits. In addition, to the best of our knowledge, we are also the first to find that universal adversarial training can be leveraged to improve clean model performance.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        P. Chiang, Y. Zhou, Omid Poursaeed, S. Shukla, T. Goldstein, S. Lim
        
      </div>

      <div class="pub-publication">
        Under Review,&nbsp;2023.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/UniAT.pdf" target="_blank" rel="noopener">
  PDF
</a>















      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/fine-grained-unrestricted/">
        <img src="/img/faces2.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/fine-grained-unrestricted/" itemprop="url">Robustness and Generalization via Generative Adversarial Training</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Several defenses have been proposed to improve robustness of deep neural networks against input variations. However, current defenses can only withstand the specific attack used in training and often degrade performance of the model on clean images. In this paper, we present an approach to simultaneously improve the model&rsquo;s generalization and robustness to unseen adversarial attacks. Instead of altering a single pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. We show that adversarial training with our approach not only improves performance of the model on clean images but also makes it robust against unforeseen attacks.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, Ser-Nam Lim
        
      </div>

      <div class="pub-publication">
        ICCV,&nbsp;2021.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAT.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/fine-grained-unrestricted.bib">
  Cite
</button>








<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAT_slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2109.02765" target="_blank" rel="noopener">
  Arxiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAT_supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/hybrid/">
        <img src="/img/hybrid.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/hybrid/" itemprop="url">Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a novel neural architecture for representing 3D surfaces by harnessing complementary explicit and implicit shape representations. We make these two representations synergistic by introducing novel consistency losses. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir Kim
        
      </div>

      <div class="pub-publication">
        ECCV,&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/HybridNet.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/hybrid.bib">
  Cite
</button>








<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Hybrid.pdf" target="_blank" rel="noopener">
  Slides
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/file/d/1wwnp6HlDdfYw19__ESxWdTQfmc8Bf--v/view?usp=sharing" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2007.10294" target="_blank" rel="noopener">
  Arxiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/HybridNet_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/few-shot/">
        <img src="/img/Few_shot.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/few-shot/" itemprop="url">Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Davis Wertheimer, Omid Poursaeed, Bharath Hariharan
        
      </div>

      <div class="pub-publication">
        ICLR,&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Few_shot.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/few-shot.bib">
  Cite
</button>











<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Few_shot_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/pdf/2011.13026.pdf" target="_blank" rel="noopener">
  arXiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/self-supervised/">
        <img src="/img/self_supervised.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/self-supervised/" itemprop="url">Self-supervised Learning of Point Clouds via Orientation Estimation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        While deep neural networks have achieved impressive results on point cloud learning tasks, they require massive amounts of manually labeled data. In this paper we leverage 3D self-supervision for learning downstream tasks on point clouds with fewer labels. A point cloud can be rotated in infinitely many ways, which provides a rich label-free source for self-supervision. We consider the auxiliary task of predicting rotations that in turn leads to useful features for other tasks. Using experiments on ShapeNet and ModelNet, we demonstrate that our approach outperforms the state-of-the-art. Moreover, features learned by our model are complementary to other self-supervised methods and combining them leads to further performance improvement.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, Vladimir Kim
        
      </div>

      <div class="pub-publication">
        3DV,&nbsp;2020.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Self_supervised_Point_Clouds.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/self-supervised.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/OmidPoursaeed/Self_supervised_Learning_Point_Clouds" target="_blank" rel="noopener">
  Code
</a>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Self_supervised_slides.pdf" target="_blank" rel="noopener">
  Slides
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/file/d/1zUVIpr_AljqTHWb52kxW6o7BbHuP8VRS/view?usp=sharing" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/2008.00305" target="_blank" rel="noopener">
  Arxiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/generative-neural-networks-for-2.5d-character-animation/">
        <img src="/img/Neural_Puppet.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/generative-neural-networks-for-2.5d-character-animation/" itemprop="url">Neural Puppet: Generative Layered Cartoon Characters</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a learning based method for generating new animations of a cartoon character given a few example images. We express pose changes as a deformation of a layered 2.5D template mesh, and devise a novel architecture that learns to predict mesh deformations matching the template to a target image. In addition to coarse poses, character appearance also varies due to shading, out-of-plane motions, and artistic effects. We capture these subtle changes by applying an image translation network to refine the mesh rendering. Our generative model can be used to synthesize in-between frames and to create data-driven deformation. Our template fitting procedure outperforms state-of-the-art generic techniques for detecting image correspondences.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Vladimir Kim, Eli Shechtman, Jun Saito, Serge Belongie
        
      </div>

      <div class="pub-publication">
        WACV,&nbsp;2019.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/generative-neural-networks-for-2.5D-character-animation.bib">
  Cite
</button>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet_Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1910.02060" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Neural_Puppet_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/differential-privacy/">
        <img src="/img/DP.png" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/differential-privacy/" itemprop="url">Differential Privacy has Disparate Impact on Model Accuracy</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Differential privacy (DP) is a popular mechanism for training machine learning models with bounded leakage about the presence of specific points in the training data. The cost of differential privacy is a reduction in the model’s accuracy. We demonstrate that in the neural networks trained using differentially private stochastic gradient descent (DP-SGD), this cost is not borne equally: accuracy of DP models drops much more for the underrepresented classes and subgroups. We demonstrate this effect for a variety of tasks and models, including sentiment analysis of text and image classification. We then explain why DP training mechanisms such as gradient clipping and noise addition have disproportionate effect on the underrepresented and more complex subgroups.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Eugene Bagdasaryan, Omid Poursaeed, Vitaly Shmatikov
        
      </div>

      <div class="pub-publication">
        NeurIPS,&nbsp;2019.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Differential_privacy.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/differential-privacy.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/ebagdasa/differential-privacy-vs-fairness" target="_blank" rel="noopener">
  Code
</a>






<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/neurips_diffprivacy_fairness.pdf" target="_blank" rel="noopener">
  Poster
</a>






      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/deep-fundamental-matrix-estimation-without-correspondences/">
        <img src="/img/Deep-FM.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/deep-fundamental-matrix-estimation-without-correspondences/" itemprop="url">Deep Fundamental Matrix Estimation without Correspondences</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Estimating fundamental matrices is a classic problem in computer vision. Traditional methods rely heavily on the correctness of estimated key-point correspondences, which can be noisy and unreliable. As a result, it is difficult for these methods to handle image pairs with large occlusion or significantly different camera poses. In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences. New modules and layers are introduced in order to preserve mathematical properties of the fundamental matrix as a homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance of the proposed model on the KITTI dataset, and show that they achieve competitive performance with traditional methods without the need for extracting correspondences.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Guandao Yang, Aditya Prakash, Hanqing Jiang, Qiuren Fang, Bharath Hariharan, Serge Belongie
        
      </div>

      <div class="pub-publication">
        ECCV,&nbsp;2018.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Deep_Fundamental_Matrix_Estimation.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/deep-fundamental-matrix-estimation-without-correspondences.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/file/d/1vcvzmZ0diUS0tpt20_TDwQ-Tl3XgG41m/view?usp=sharing" target="_blank" rel="noopener">
  Code
</a>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/F_Matrix_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/pdf/1810.01575.pdf" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/open?id=1sYJsb9Aa2E6cXxzelSNZtdjCQgwuKczy" target="_blank" rel="noopener">
  Poster
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/generative-adversarial-perturbations/">
        <img src="/img/GAP_Architecture.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/generative-adversarial-perturbations/" itemprop="url">Generative Adversarial Perturbations</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool trained models. Our approach can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. We improve the state-of-the-art performance in universal perturbations by leveraging generative models in lieu of current iterative methods. Our attacks are considerably faster than iterative and optimization-based methods at inference time. Moreover, we are the first to present effective targeted universal perturbations.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie
        
      </div>

      <div class="pub-publication">
        CVPR,&nbsp;2018.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/generative-adversarial-perturbations.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/OmidPoursaeed/Generative_Adversarial_Perturbations" target="_blank" rel="noopener">
  Code
</a>







<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAP_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1712.02328" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/open?id=1fFIJM2ETysPmAfra7qRA3Wc25cZuIrnD" target="_blank" rel="noopener">
  Poster
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/GAP_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/stacked-generative-adversarial-networks/">
        <img src="/img/sgan.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/stacked-generative-adversarial-networks/" itemprop="url">Stacked Generative Adversarial Networks</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a novel generative model which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie
        
      </div>

      <div class="pub-publication">
        CVPR,&nbsp;2017.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/SGAN_CVPR.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/stacked-generative-adversarial-networks.bib">
  Cite
</button>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/xunhuang1995/SGAN" target="_blank" rel="noopener">
  Code
</a>






<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/SGAN_poster.pdf" target="_blank" rel="noopener">
  Poster
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/SGAN_Slides.pdf" target="_blank" rel="noopener">
  Slides
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1612.04357" target="_blank" rel="noopener">
  ArXiv
</a>


      </div>

    </div>
  </div>
</div>

      
    
      
        <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://omidpoursaeed.github.io/publication/vision-based-real-estate-price-estimation/">
        <img src="/img/vision-based-narrow-2.jpg" class="pub-banner"
             itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://omidpoursaeed.github.io/publication/vision-based-real-estate-price-estimation/" itemprop="url">Vision Based Real Estate Price Estimation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Several online real estate database companies provide automatic estimation of market values for houses using a proprietary formula. Although these estimates are often close to the actual sale prices, in some cases they are highly inaccurate. One of the key factors that affects the value of a house is its interior and exterior appearance, which is not considered in calculating these estimates. In this paper, we evaluate the impact of visual characteristics of a house on its market value. Using deep convolutional neural networks on a large dataset of photos of home interiors and exteriors, we develop a novel framework for automated value assessment using these photos in addition to other home characteristics. By applying our proposed method for price estimation to a new dataset of real estate photos and metadata, we show that it outperforms Zillow&rsquo;s estimates.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Omid Poursaeed, Tomas Matera, Serge Belongie
        
      </div>

      <div class="pub-publication">
        Machine Vision and Applications,&nbsp;2017.
      </div>

      <div class="pub-links">
        




<a class="btn btn-primary btn-outline btn-xs" href="http://rdcu.be/KzC9" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/vision-based-real-estate-price-estimation.bib">
  Cite
</button>



<a class="btn btn-primary btn-outline btn-xs" href="https://drive.google.com/file/d/0BxDIywue_VABY1dRcFVvZ3BodnM/view?usp=sharing" target="_blank" rel="noopener">
  Dataset
</a>









<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1707.05489" target="_blank" rel="noopener">
  ArXiv
</a>

<a class="btn btn-primary btn-outline btn-xs" href="https://omidpoursaeed.github.io/pdf/Vision_based_Supp.pdf" target="_blank" rel="noopener">
  Supp
</a>


      </div>

    </div>
  </div>
</div>

      
    

  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="experience" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Experience</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <ul>
<li>Research Intern: <strong>Adobe Research</strong> with <a href="http://www.vovakim.com/" target="_blank">Vladimir Kim</a> and <a href="https://techmatt.github.io" target="_blank">Matthew Fisher</a>, Summer 2019 <br />
<br /></li>
<li>Research Intern: <strong>Facebook AI</strong> with <a href="https://www.linkedin.com/in/sernam/" target="_blank">SerNam Lim</a>, Fall &amp; Winter 2019 <br />
<br /></li>
<li>Research Intern: <strong>Adobe Research</strong> with <a href="http://www.vovakim.com/" target="_blank">Vladimir Kim</a> and <a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a>, Summer 2018 <br />
<br /></li>
<li>Research Intern: <strong>eBay Research</strong> with <a href="http://kylezheng.org/" target="_blank">Shuai Zheng</a>, <a href="http://www.cs.unc.edu/~hadi/" target="_blank">Hadi Kiapour</a> and <a href="https://www.linkedin.com/in/rpiramuthu/" target="_blank">Robinson Piramuthu</a>, Summer 2017</li>
</ul>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="talks" class="home-section">
    <div class="container">
      




<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Presentations</h1>
    
    
  </div>
  <div class="col-xs-12 col-md-8">
    <p><ul>
<li><p><a href="https://docs.google.com/presentation/d/1aRHXatmiHcXutxX88LkqGyA8AFnWBRb3Gc2DYbJvGCA/edit?usp=sharing" target="_blank">Hybrid Generative Models for 2D and 3D Computer Vision</a>: Invited talk at Facebook AI and Apple</p></li>

<li><p><a href="https://docs.google.com/presentation/d/1Z-Srx-elUDYD6iKnHF6F5XN5z_ZzKAqQG_U_kw7-Q4M/edit?usp=sharing" target="_blank">Learning to Generate and Animate for Augmented Reality</a>: Invited talk at Facebook Reality Labs and Microsoft Mixed Reality &amp; AI Labs</p></li>

<li><p><a href="https://docs.google.com/presentation/d/1QjcOEsWqPhBh5MvL1VwCc_ti9VPLJg4FJUgkZOg7TjU/edit?usp=sharing" target="_blank">DeepFakes and Adversarial Examples: Fooling Humans and Machines</a>: <a href="https://36ef64cb-137f-4a56-9f1f-edccbcdef5aa.usrfiles.com/ugd/36ef64_a3ebec11f15c4b12b71db78c087a5f2e.pdf" target="_blank">Digital Life Seminar Series</a></p></li>
</ul>
</p>

    
  </div>
</div>

    </div>
  </section>
  

  
  
  
  <section id="professional_service" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Service</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <p>Reviewer for CVPR, NeurIPS, ICCV, ECCV, ICLR, AAAI, IEEE TPAMI, IEEE Transactions on Multimedia, IEEE Transactions on Information Forensics &amp; Security, ACM Transactions on Privacy and Security, IEEE Transactions on Industrial Electronics, CVPR Workshop on Adversarial Machine Learning</p>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="patents" class="home-section">
    <div class="container">
      


<div class="row">
  
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Patents</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <ul>
<li><a href="https://patents.google.com/patent/US20210264659A1/en" target="_blank">Learning Hybrid Shape Representations</a>, with Vladimir Kim, Matthew Fisher and Noam Aigerman
<br />
<br /></li>
<li><a href="https://patents.google.com/patent/US20200265294A1/en" target="_blank">Object animation using Generative Neural Networks</a>, with Vladimir Kim, Jun Saito and Eli Shechtman
<br />
<br /></li>
<li><a href="https://patents.google.com/patent/US20190286950A1/en" target="_blank">Generating a digital image using a generative adversarial network</a>, with Shuai Zheng, Hadi Kiapour and Robinson Piramuthu</li>
</ul>

  </div>
  
</div>

    </div>
  </section>
  

  
  
  
  <section id="contact" class="home-section">
    <div class="container">
      




<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Contact</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    

    <ul class="fa-ul" itemscope>

      
      <li>
        <i class="fa-li fa fa-envelope fa-2x" aria-hidden="true"></i>
        <span id="person-email" itemprop="email"><a href="mailto:op63@cornell.edu">op63@cornell.edu</a></span>
      </li>
      

      

      

      

      

      

      
      <li>
        <i class="fa-li fa fa-map-marker fa-2x" aria-hidden="true"></i>
        <span id="person-address" itemprop="address">390 9th Ave, New York, NY 10001, USA</span>
      </li>
      

      

    </ul>

    

  </div>
</div>

    </div>
  </section>
  



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Omid Poursaeed &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>


